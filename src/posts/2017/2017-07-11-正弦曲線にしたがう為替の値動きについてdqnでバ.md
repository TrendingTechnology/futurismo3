---
author: admin
categories:
- Python
- 機械学習
date: 2017-07-11T14:02:09+00:00
dsq_thread_id:
- 5.9806705e+09
excerpt: 正弦曲線にしたがう為替の値動きについてDQNでバックテストをしてみた
follow:
- follow
fullscreen_view:
- "n"
index:
- index
menu_view:
- "y"
page_layout:
- def
pdrp_attributionLocation:
- end
pvc_views:
- 747
side:
- "y"
tags:
- DeepLearning
- FX
- 強化学習
title: 正弦曲線にしたがう為替の値動きについてDQNでバックテストをしてみた
title_view:
- "y"
type: post
url: /archives/=6615
---

DQNを勉強しているので、以下の記事に触発されて自分もFXでDQNをしてみたくなった。

  * <https://github.com/dogwood008/DeepFX>
  * <http://recruit.gmo.jp/engineer/jisedai/blog/deep-q-learning/>
  * <https://www.slideshare.net/JunichiroKatsuta/deep-qlearningfx>

2年前、OANDAでのシステムトレードで5万円を損した屈辱を今こそ晴らすのだ！

  * [夏休みの自由研究 は OANDA APIを利用して FX システムトレード | Futurismo][1]

まずは、GMOの記事でも実施しているように、正弦曲線に従う為替の値動きについて、
  
DQNを適用してバックテストを行ってみた。

今回のJupyter Notebookは Gistにあげました。

  * <https://gist.github.com/tsu-nera/1102438625def2b4a9f6e5a5d8f3f236>

### 正弦曲線 {#-}

以下のような正弦曲線について、DQNでバックテストを試みる。

![][2]

### MDP {#mdp}

  * 状態 
      * (-1.0, 1.0) の範囲の値
      * 売買のステータス(SELL, NONE, BUY)
  * 行動 
      * SELL
      * HOLD
      * BUY
  * 報酬 
      * 売買の即時報酬

### コード {#-}

    `import enum
    class Action(enum.Enum):
        SELL = 2
        HOLD = 0
        BUY  = 1
    
    class Position():
        def __init__(self):
            self.NONE = 0
            self.BUY = 1
            self.SELL = 2
    
            self.bid = 0.0
            self.pos_state = self.NONE
    
        def state(self):
            return np.array([self.bid, self.pos_state])
    
        def change(self, action, bid):
            self.bid = bid
            self.pos_state = action
    
        def close(self, action, bid):
            if self.pos_state == action:
                return 0
    
            self.pos_state = self.NONE
            if action == Action.BUY.value:
                reward = (bid - self.bid) * 1000
            else:
                reward = (self.bid - bid) * 1000
            return reward`
    

keras-rl を利用するために、環境を OpenAI gym ライクで作成する。

    `import gym
    import gym.spaces
    
    class FXTrade(gym.core.Env):
        def __init__(self):
            self.action_space = gym.spaces.Discrete(3)
                high = np.array([1.0, 1.0])
                self.observation_space = gym.spaces.Box(low=-high, high=high)
    
                self._position = Position()
    
                self._sin_list = []
                for t in np.linspace(0, 48, 240):
                    self._sin_list.append(np.sin(t))
                self.cur_id = 0
    
        def _step(self, action):
            bid = self._sin_list[self.cur_id]
            self.cur_id +=1
            done = True if self.cur_id == 240 else False
    
            if action == Action.HOLD.value:
                reward = 0
            else:
                if self._position.pos_state == self._position.NONE:
                    self._position.change(action, bid)
                    reward = 0
                else:
                    reward = self._position.close(action, bid)
            return np.array([bid, self._position.pos_state]), reward, done ,{}
    
        def _reset(self):
            self.cur_id = 0
            self._position = Position()
            return np.array([0.0, 0.0])`
    

### keras-rlの登場 {#keras-rl-}

強化学習を笑っちゃうほど簡単に実施するために Keras-rlをつかう。

  * <https://github.com/matthiasplappert/keras-rl>

このライブラリは抽象度が高すぎてもはやなにをやっているのかわからない。
  
exampleを参考にして、それっぽいコードを書いてみる。

    `from keras.models import Sequential
    from keras.layers import Dense, Activation, Flatten
    from keras.optimizers import Adam
    
    from rl.agents.dqn import DQNAgent
    from rl.policy import BoltzmannQPolicy
    from rl.memory import SequentialMemory
    
    nb_actions = env.action_space.n
    
    model = Sequential()
    model.add(Flatten(input_shape=(1,) + env.observation_space.shape))
    model.add(Dense(16))
    model.add(Activation('relu'))
    model.add(Dense(16))
    model.add(Activation('relu'))
    model.add(Dense(16))
    model.add(Activation('relu'))
    model.add(Dense(nb_actions))
    model.add(Activation('linear'))
    model.summary()
    
    # DQNに必要な Experience Replayのためのメモリ
    memory = SequentialMemory(limit=50000, window_length=1)
    # ボルツマン分布に従うポリシー
    policy = BoltzmannQPolicy()
    # DQN Agentの生成
    dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory,
                   target_model_update=1e-2, policy=policy)
    # optimizersはAdam, 損失関数は 平均二乗誤差
    dqn.compile(Adam(lr=1e-3), metrics=['mae'])
    
    # トレーニングを開始。同じ正弦曲線を9600 = 240 x 400回 回す。
    dqn.fit(env, nb_steps=9600, visualize=False, verbose=1)
    
    # トレーニング結果を確認
    dqn.test(env, nb_episodes=5, visualize=False)
    

\`
  
おお、rewardがプラスだ。。

    `Testing for 5 episodes ...
    Episode 1: reward: 603.962, steps: 240
    Episode 2: reward: 603.962, steps: 240
    Episode 3: reward: 603.962, steps: 240
    Episode 4: reward: 603.962, steps: 240
    Episode 5: reward: 603.962, steps: 240`
    

### おわりに {#-}

  * 理論を抑えていないので何をやっているのかがいまいちわからないのだけれども、おそらく上がり調子のときは報酬もたくさんもらえるので、買い注文を多くしているのだと思う。
  * keras-rlは非常に強力なライブラリだけれども、抽象度が高すぎてなにやってるのかよくわからない。理解を深めるために numpyで実装してみるのもありかもしれない。
  * 状態は、その時の値のみを扱ったが、過去５bin分の状態を考慮にいれたらどうなるだろうか？いわゆるwindowの概念を強化学習にいれてみると、結果がよくなるだろうか？

 [1]: http://futurismo.biz/archives/4392
 [2]: https://lh3.googleusercontent.com/-met0o3s4QQCXzzdygoYDeVRdmRWr1Mhw5cT_nvMvmkGLBDUy1NJCZ9We4RmO_Mlg7fkIw-d133CkSSrBjtM5oByuNkH55Tiv634IFUpY9N1z3VZpjFvw2fQTNlbUmCqC1ZhENj11KYujk4vk3gUXYgOoIRjGsb7tOJO6sX1yROfPraGoZ1PSG3ZgARbgZy8aUXUB8SchXHYgT81X1wn4QL87QhB722B8X0MUWJRpI_KZXl4v_wYeOe5x0ZnRD_5gDfMHmLDedNBuqYIfejMB-9Kucx0inOOEvH5lPp64YOiL8gZ-CF626z3VdO4vLD77rBxmde8OlQzPv6r07vKvFwEjEDjzM7hpW-bPwKJnR0QKVbpx4Lcpo9IkjJSNmvqhBtX3YDhl2fr-14E2-Ymp1Tu-8-cxAlJ6cMB0qEtphzWgnVPuqJWeugVNyszv6qDFeL_uVlbspH8anHVNhGzQs9XmztziRmnO7ckOXjBCm9DLSNOn6Tpkf2Y-tcFeQu5SlXj4Th1Z8Urg1dwXzZLybvWWKo1XxLd0vl3Bv9duyOkVxNeSbt-ybD2HSiCHFG-XxCWDtUd86ADpsZ39CWq51Mj1AwRloRF30Ha88Y1Dtpeu3cTTxm-598q=w390-h252-no